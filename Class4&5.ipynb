{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/junkyuhufs/Class2023Spring/blob/main/Class4%265.ipynb",
      "authorship_tag": "ABX9TyOlnU7EKULFVX1lohzlKsCB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junkyuhufs/Class2024Spring/blob/main/Class4%265.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Natural Language Processing (NLP)\n",
        "##Text Processing\n",
        "\n",
        "by virtue of Dr. Hosung Nam"
      ],
      "metadata": {
        "id": "DSpZNqeZg6oW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text processing"
      ],
      "metadata": {
        "id": "aRr7vYOJgTW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = 'No pain no gain'"
      ],
      "metadata": {
        "id": "ZhiNGnUAgXcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'pain' in s"
      ],
      "metadata": {
        "id": "L41WK3pigioT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s.split() #split > 공백기준 자르기"
      ],
      "metadata": {
        "id": "d0eLi7YZgphF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s.split().index('No')"
      ],
      "metadata": {
        "id": "lV39sc7Eg9PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s[-4:] #마지막에서 4번째 character까지"
      ],
      "metadata": {
        "id": "U1io_U_xhXse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s.split()[1]"
      ],
      "metadata": {
        "id": "OQhoBxjPiJuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##대소문자\n",
        "내장함수 lower(), upper()이용"
      ],
      "metadata": {
        "id": "fIiWBfaSiplb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 'ABcdEfg'\n",
        "\n",
        "소문자 = k.lower()\n",
        "대문자 = k.upper()\n",
        "\n",
        "print(소문자, 대문자)"
      ],
      "metadata": {
        "id": "kOA6nS55is65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##정규화(Normalization)"
      ],
      "metadata": {
        "id": "EzghLGAGlA9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = 'John visited US on 22-9-21.'\n",
        "s"
      ],
      "metadata": {
        "id": "fWa-OnZnlEoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_normal = s.replace('US', 'United States').replace('-21', '-2021')\n",
        "s_normal"
      ],
      "metadata": {
        "id": "CnaeXb0nlSiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Spelling correction"
      ],
      "metadata": {
        "id": "c5of_VMX7zYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autocorrect\n",
        "from autocorrect import Speller"
      ],
      "metadata": {
        "id": "f_IuF5iy72rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spell = Speller('en')\n",
        "\n",
        "print(spell('anttomatic'))\n",
        "print(spell('auomatic'))\n",
        "print(spell('automatik'))"
      ],
      "metadata": {
        "id": "cKCOQOAi8HcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "jb1K-QSGzHEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = word_tokenize('A bad workman alwayss blames his tols.')\n",
        "print(s)\n",
        "s_correct = ' '.join([spell(s) for s in s])\n",
        "print(s_correct)"
      ],
      "metadata": {
        "id": "jHquKUw989iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Singular, plural"
      ],
      "metadata": {
        "id": "MSR-8_Gw_EHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import textblob\n",
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "ijaVkuQBPHG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = 'books bananas pineapples'\n",
        "tb = TextBlob(words)\n",
        "print(tb.words)\n",
        "print(tb.words.singularize())"
      ],
      "metadata": {
        "id": "ZTF76xx0_Io2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = 'book banana pineapple'\n",
        "tb = TextBlob(words)\n",
        "print(tb.words)\n",
        "print(tb.words.pluralize())"
      ],
      "metadata": {
        "id": "03Xxb3giAE_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenization"
      ],
      "metadata": {
        "id": "fhGzdX6PhFg5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word tokenization\n",
        "내장함수 split 이용가능(공백기준 자르기)\n"
      ],
      "metadata": {
        "id": "O7gRg2ghmNFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = 'Life is either a daring adventure or nothing at all.'\n",
        "tokens = [x for x in m.split(' ')]\n",
        "tokens"
      ],
      "metadata": {
        "id": "omV3t-lImQRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "또는 NLTK의 word tokenize이용"
      ],
      "metadata": {
        "id": "BK59M8Pcnunm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "MLBijg9gg5x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tokens2 = word_tokenize(m)\n",
        "tokens2"
      ],
      "metadata": {
        "id": "o8Ixo6QOnhOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regular expression이용"
      ],
      "metadata": {
        "id": "YK3RlmXC7fPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "retokenize = RegexpTokenizer(\"[\\w]+\") #punctuation도 제거함; 그나마 이게 가장 나음; \\w >> 단어전체 ; + >> 공백\n",
        "token3 = retokenize.tokenize(m)\n",
        "token3"
      ],
      "metadata": {
        "id": "kamNxdDM7R6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence tokenization\n",
        "split이용 > 줄바꿈문자 ('\\n') 기준으로 문장분리"
      ],
      "metadata": {
        "id": "13zqGRdbolfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sen = 'Life is either a daring adventure or nothing at all.\\nThey must often change who would be constant in happiness or wisdom.'\n",
        "\n",
        "tokens4 = [x for x in sen.split('\\n')]\n",
        "tokens4"
      ],
      "metadata": {
        "id": "hMRVVL0GoofO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nltk의 sent_tokenize() 이용"
      ],
      "metadata": {
        "id": "imqqYiXrqRyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "token5 = sent_tokenize(sen)\n",
        "token5"
      ],
      "metadata": {
        "id": "QVQOb-6IqQoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras 이용"
      ],
      "metadata": {
        "id": "ZvzY2vQTizXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "sent = 'Wherer there\\'s a will, there\\'s a way'\n",
        "text_to_word_sequence(sent)"
      ],
      "metadata": {
        "id": "iwypvri8i2D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Textblob 이용"
      ],
      "metadata": {
        "id": "g5IaW4t-kYeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from textblob import TextBlob\n",
        "sent = 'Wherer there\\'s a will, there\\'s a way'\n",
        "blob = TextBlob(sent)\n",
        "blob.words"
      ],
      "metadata": {
        "id": "BkgYCfxOkbjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cf. MWETokenizer >> Multi-words experssion을 고려해서 tonkenization"
      ],
      "metadata": {
        "id": "YUs7cwOZlUck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "tokenizer = MWETokenizer()\n",
        "tokenizer.add_mwe(('the', 'west', 'wing'))\n",
        "tokenizer.tokenize('Something about the west wing'.split())"
      ],
      "metadata": {
        "id": "HGfFOgKJKvq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "mwetokenizer = MWETokenizer([('President','of','the','United','States')], separator=' ')\n",
        "mwetokenizer.add_mwe(('President','of','France'))"
      ],
      "metadata": {
        "id": "xfAIYE8jLnzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "sentence = \"Trump is the President of the United States, and Macron is the President of France.\"\n",
        "mwetokenized_sentence = mwetokenizer.tokenize(word_tokenize(sentence))"
      ],
      "metadata": {
        "id": "gndtNIH7Lqx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mwetokenized_sentence)"
      ],
      "metadata": {
        "id": "b6WIZSwJLxqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "6Zi4VGSMMMHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_sentence = [token for token in mwetokenizer.tokenize(word_tokenize(sentence)) if token not in stop_words]\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "id": "W5plog0aL9yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## n-gram 추출\n"
      ],
      "metadata": {
        "id": "Omi3L8J9lgGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "sent = 'The beginning is half of the whole.'\n",
        "bigram = list(ngrams(sent.split(), 2))\n",
        "print(bigram)"
      ],
      "metadata": {
        "id": "I1_ekwRMlqK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "sent = 'The beginning is half of the whole.'\n",
        "trigram = list(ngrams(sent.split(), 3))\n",
        "print(trigram)"
      ],
      "metadata": {
        "id": "UvGbXlvfmp0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "blob = TextBlob(sent)\n",
        "blob.ngrams(n=2)"
      ],
      "metadata": {
        "id": "Cd_kzSXDnAdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blob.ngrams(n=3)"
      ],
      "metadata": {
        "id": "wb5V4mO4nbMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##파일 불러오기"
      ],
      "metadata": {
        "id": "BD3oiHr46avs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy a file from github\n",
        "import os\n",
        "url = \"https://raw.githubusercontent.com/junkyuhufs/Class2023Spring/main/crime_punishment.txt\"\n",
        "os.system(\"curl \" + url + \" > crime_punishment.txt\")\n",
        "\n",
        "# read a text file in the server\n",
        "file = open(\"crime_punishment.txt\")   #텍스트파일을 open해서 file에 담아라\n",
        "text2 = file.read().replace(\"\\n\", \" \") # text2에 file의 모든 내용을 담아라 (string으로 담음); Wn >> enter를 없애라\n",
        "file.close()"
      ],
      "metadata": {
        "id": "otPEgwGeU9w5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# or copy/pase text here\n",
        "text = 'Here’s to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. \\\n",
        "The ones who see things differently — they’re not fond of rules. \\\n",
        "You can quote them, disagree with them, glorify or vilify them, \\\n",
        "but the only thing you can’t do is ignore them because they change things. \\\n",
        "They push the human race forward, and while some may see them as the crazy ones, we see genius, \\\n",
        "because the ones who are crazy enough to think that they can change the world, are the ones who do.'"
      ],
      "metadata": {
        "id": "EKFWSLaVh3Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write out a text file\n",
        "file = open(\"tmp.txt\", \"w\")  #tmp.txt로 파일생성(text를 저장; w는 https://reakwon.tistory.com/174 참조)\n",
        "file.write(text)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "kM7s4QG_h-7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text.split() #띄어쓰기 기준으로 자름; 가장 원식적인 tokenization"
      ],
      "metadata": {
        "id": "uvfrsPVniH6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join(text.split()) # space로 합쳐라"
      ],
      "metadata": {
        "id": "OroSzOggiPA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt') #puctuation기반 자르기; punctuation남아있음\n",
        "words = word_tokenize(text)"
      ],
      "metadata": {
        "id": "u4kbGcdCiRKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "id": "uZQw6Y8AB0yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "retokenize = RegexpTokenizer(\"[\\w]+\") #punctuation도 제거함; 그나마 이게 가장 나음\n",
        "words = retokenize.tokenize(text)"
      ],
      "metadata": {
        "id": "0Xghdzd2iWxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "id": "Vg_YG4bwCU7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Normalization (kinds of data cleaning)\n",
        "**Stemming** 어간 추출 대충의 패턴 규칙으로 어미를 잘라내는 것 (사전에 없는 어간 나올 수 있음)\n",
        "\n",
        "**Lemmatization** 표제어(기본 사전형) 추출."
      ],
      "metadata": {
        "id": "zNYkM9_Nijrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "[stemmer.stem(w) for w in words]"
      ],
      "metadata": {
        "id": "Mmz_HmtoinlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "[stemmer.stem(w) for w in words]"
      ],
      "metadata": {
        "id": "IO3LYXiEiteP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization**"
      ],
      "metadata": {
        "id": "eQZBDtMIGTG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "[lemmatizer.lemmatize(w) for w in words]"
      ],
      "metadata": {
        "id": "AJneawkHDrN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
        "\n",
        "# a denotes adjective in \"pos\"\n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))"
      ],
      "metadata": {
        "id": "DtI4eurlxfJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stopwords"
      ],
      "metadata": {
        "id": "m1JmfPYjzoj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "수동으로 제작"
      ],
      "metadata": {
        "id": "BWfQJSSwtELd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = \"on in the\"\n",
        "stop_words = stop_words.split(' ')\n",
        "stop_words"
      ],
      "metadata": {
        "id": "AtHbLYdWqIqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = 'Actors on the stage'\n",
        "sent = sent.split(' ')\n",
        "nouns =[]\n",
        "for noun in sent:\n",
        "  if noun not in stop_words:\n",
        "    nouns.append(noun)\n",
        "\n",
        "nouns"
      ],
      "metadata": {
        "id": "i7fTpdnqqdl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nltk 불용어 리스트 이용"
      ],
      "metadata": {
        "id": "7JwGrmXgqCNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "KDBxik6gtS4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "print(stop_words)"
      ],
      "metadata": {
        "id": "GCc2bTRe2qr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"Put off for one day and ten days will pass.\"\n",
        "words = word_tokenize(s)\n",
        "print(words)"
      ],
      "metadata": {
        "id": "9d_yROga3UpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_stopwords = []\n",
        "for w in words:\n",
        "  if w not in stop_words:\n",
        "    no_stopwords.append(w)\n",
        "\n",
        "print(no_stopwords)"
      ],
      "metadata": {
        "id": "pEo84KOS4y3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_stopword = [w for w in words if not w in stopwords.words('english')] #words에 있는 단어들 중 하나씩 반복해서 w에 담아라. 불용어에 포함되지 않으면.\n",
        "print(no_stopword) #불용어 처리후"
      ],
      "metadata": {
        "id": "xwviblAnzr_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Collocation, Concordance"
      ],
      "metadata": {
        "id": "rbrlvMvmjwxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('gutenberg')\n",
        "text = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
        "words = retokenize.tokenize(text)"
      ],
      "metadata": {
        "id": "0ypb8fXsjyCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(text)"
      ],
      "metadata": {
        "id": "Q4uYCiHx1bIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[0:1000]"
      ],
      "metadata": {
        "id": "QVZ4CNRF1trI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "id": "-PAuU4at10R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(words)"
      ],
      "metadata": {
        "id": "YvAehGYE2Jyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words[0:100]"
      ],
      "metadata": {
        "id": "qq5WlpLp2PDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "id": "mcfV_NJO2Uhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.Text(words).collocations()  # default: (num=20, window_size=2)"
      ],
      "metadata": {
        "id": "CjLqFQ6Mj1YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.Text(words).concordance('Emma', 79, 10) # 79 = character숫자, 10 = 문장갯수"
      ],
      "metadata": {
        "id": "uvCyUYI2j9Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.Text(words).dispersion_plot([\"Knightley\", \"Frank\", \"Jane\", \"Harriet\", \"Robert\", \"Emma\"])"
      ],
      "metadata": {
        "id": "og7UCpsdkD0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distributional similarity:\n",
        "# find other words which appear in the same contexts as the specified word;\n",
        "# list most similar words first.\n",
        "nltk.Text(words).similar(\"Emma\")"
      ],
      "metadata": {
        "id": "6PYq78sTkJea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find contexts where the specified words appear; list most frequent common contexts first.\n",
        "nltk.Text(words).common_contexts([\"Emma\", \"she\"])"
      ],
      "metadata": {
        "id": "51Qkvu4IkPFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Frequency distribution, Frequency plot"
      ],
      "metadata": {
        "id": "jkjbVWVgkT_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fd = nltk.FreqDist(words).most_common(20)\n",
        "fd"
      ],
      "metadata": {
        "id": "mEs7z3nzkVka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.Text(words).plot(20)"
      ],
      "metadata": {
        "id": "cxsXvjvDkZrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stop words를 삭제한 후 빈도분포와 빈도 도표"
      ],
      "metadata": {
        "id": "WKe1HXcFjKoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_stop_gutenberg = [w for w in words if not w in stopwords.words('english')]"
      ],
      "metadata": {
        "id": "Hu3Fe7Z5iSVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fd_nostop = nltk.FreqDist(no_stop_gutenberg).most_common(20)\n",
        "fd_nostop"
      ],
      "metadata": {
        "id": "pQYEsvhmidbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.Text(no_stop_gutenberg).plot(20)"
      ],
      "metadata": {
        "id": "kpohR5kvipgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dictionary"
      ],
      "metadata": {
        "id": "cB16-wnWmZJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')\n",
        "nltk.corpus.words.words('en')[-20:-1] #[-20:-1] >> 제일 마지막 20개"
      ],
      "metadata": {
        "id": "1YBFyBCumb5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(nltk.corpus.words.words('en'))"
      ],
      "metadata": {
        "id": "WKo_PPZLmgQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extract information (pos tag, named entity)\n",
        "##POS tag list:\n",
        "CC coordinating conjunction \\ CD cardinal digit \\ DT determiner \\ EX existential there (like: \"there is\" ... think of it like \"there exists\") \\ FW foreign word \\ IN preposition/subordinating conjunction \\ JJ adjective 'big' \\ JJR adjective, comparative 'bigger' \\ JJS adjective, superlative 'biggest' \\ LS list marker 1) \\ MD modal could, will \\ NN noun, singular 'desk' \\ NNS noun plural 'desks' \\ NNP proper noun, singular 'Harrison' \\ NNPS proper noun, plural 'Americans' \\ PDT predeterminer 'all the kids' \\ POS possessive ending parent's \\ PRP personal pronoun I, he, she \\ PRP$ possessive pronoun my, his, hers \\ RB adverb very, silently, \\ RBR adverb, comparative better \\ RBS adverb, superlative best \\ RP particle give up \\ TO to go 'to' the store. \\ UH interjection errrrrrrrm \\ VB verb, base form take \\ VBD verb, past tense took \\ VBG verb, gerund/present participle taking \\ VBN verb, past participle taken \\ VBP verb, sing. present, non-3d take \\ VBZ verb, 3rd person sing. present takes \\ WDT wh-determiner which \\ WP wh-pronoun who, what \\ WP$ possessive wh-pronoun whose \\ WRB wh-abverb where, when \\"
      ],
      "metadata": {
        "id": "JyNjm9oVmmNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk import word_tokenize"
      ],
      "metadata": {
        "id": "CpO4Cn0nn4_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(\"I am John from America and would like to go to Starbuck.\")\n",
        "words"
      ],
      "metadata": {
        "id": "d9eTHz7qoO7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.pos_tag(words)"
      ],
      "metadata": {
        "id": "I9W0ueiwogoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Named Entity Recognition (개체명인식)"
      ],
      "metadata": {
        "id": "6ZT2YYARC9Hy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "2CpgupVhC-Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = 'Rome was not built in a day.'\n",
        "print(s)"
      ],
      "metadata": {
        "id": "F9q2qjhbDCGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = nltk.pos_tag(word_tokenize(s))\n",
        "print(tags)"
      ],
      "metadata": {
        "id": "fk6CQ8H_DFL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities = nltk.ne_chunk(tags, binary=True)\n",
        "print(entities)"
      ],
      "metadata": {
        "id": "apzPIXUwDTNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lexical Ambiguity\n",
        "\n",
        "[WordNet](https://wordnet.princeton.edu/)"
      ],
      "metadata": {
        "id": "mwNx4DT9D1gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.wsd import lesk\n",
        "\n",
        "s = 'I saw bats.'\n",
        "\n",
        "print(word_tokenize(s))\n",
        "print(lesk(word_tokenize(s), 'saw'))\n",
        "print(lesk(word_tokenize(s), 'bat'))"
      ],
      "metadata": {
        "id": "iMtvFaqED4rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Wordcloud"
      ],
      "metadata": {
        "id": "di4pA0Wcm8nA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "text = nltk.corpus.gutenberg.raw('bible-kjv.txt')\n",
        "\n",
        "wc = WordCloud().generate(text)\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "zFSWQ_2rm-Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(STOPWORDS) # set는 항목이 반복되지 않게 고정\n",
        "stopwords.add('unto') #'unto'를 stopword로 추가\n",
        "wc = WordCloud(stopwords = stopwords).generate(text)\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "OS7hrrnXnENR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regular expression"
      ],
      "metadata": {
        "id": "7To-Vz__nfo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "3gQUyoWCni2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re.search('a', 'abcdefa')"
      ],
      "metadata": {
        "id": "uQvlGwE1nlwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall('a', 'abcdefa')"
      ],
      "metadata": {
        "id": "_sEtQEPCno7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re.sub('a', 'b', 'abcdefa')"
      ],
      "metadata": {
        "id": "ilPgEqYCnsjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'''       Basic Regular Expression Meta-Characters, Including Wildcards, Ranges and Closures\n",
        "\n",
        ".\t        Wildcard, matches any character\n",
        "^abc\t    Matches some pattern abc at the start of a string\n",
        "abc$\t    Matches some pattern abc at the end of a string\n",
        "[abc]\t    Matches one of a set of characters\n",
        "[^abc]    Matches anything but a set of characters\n",
        "[A-Z0-9]\tMatches one of a range of characters\n",
        "ed|ing|s\tMatches one of the specified strings (disjunction)\n",
        "*\t        Zero or more of previous item, e.g. a*, [a-z]* (also known as Kleene Closure)\n",
        "+\t        One or more of previous item, e.g. a+, [a-z]+\n",
        "?\t        Zero or one of the previous item (i.e. optional), e.g. a?, [a-z]?\n",
        "{n}\t      Exactly n repeats where n is a non-negative integer\n",
        "{n,}\t    At least n repeats\n",
        "{,n}\t    No more than n repeats\n",
        "{m,n}\t    At least m and no more than n repeats\n",
        "a(b|c)+\t  Parentheses that indicate the scope of the operators\n",
        "(...)     Matches whatever regular expression is inside the parentheses\n",
        "\\d\n",
        "Matches any decimal digit; this is equivalent to the class [0-9].\n",
        "\\D\n",
        "Matches any non-digit character; this is equivalent to the class [^0-9].\n",
        "\\s\n",
        "Matches any whitespace character; this is equivalent to the class [ \\t\\n\\r\\f\\v].\n",
        "\\S\n",
        "Matches any non-whitespace character; this is equivalent to the class [^ \\t\\n\\r\\f\\v].\n",
        "\\w\n",
        "Matches any alphanumeric character; this is equivalent to the class [a-zA-Z0-9_].\n",
        "\\W\n",
        "Matches any non-alphanumeric character; this is equivalent to the class [^a-zA-Z0-9_].\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "q8ZOAsa-nxiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')\n",
        "\n",
        "engdict = nltk.corpus.words.words('en')\n",
        "\n",
        "result = [w for w in engdict if re.search('ed$', w)] # w라는 string에 'ed$'가 있으면; $는 string의 끝\n",
        "#result = [w for w in engdict if re.search('^..j..t..$', w)] # ^string 시작, $ string 끝, 점 > any character\n",
        "#result = [w for w in engdict if re.search('^[ghi][mno][jlk][def]$', w)] # [ghi] > 대괄호 안에 있는 ghi중 하나의 character를 만족시키면\n",
        "#result = [w for w in engdict if re.search('^[ah]+$', w)][:10] # + > 한 번 이상 반복\n",
        "print(result[:10])"
      ],
      "metadata": {
        "id": "TCheNyCwny4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('treebank')\n",
        "wsj = nltk.corpus.treebank.words()\n",
        "\n",
        "result = [w for w in wsj if re.search('(ed|ing)$', w)] # | >> OR\n",
        "#result = [w for w in wsj if re.search('^[0-9]+\\.[0-9]+$', w)] # [0-9]+ >> 숫자가 하나이상 나오고; \\. >> dot (참고. .은 any character)\n",
        "#result = [w for w in wsj if re.search('^[A-Z]+\\$$', w)] # [A-Z]+ >> 어떤 대문자가 하나이상 나오고; \\$ >> 달라표시 (첫번째 $); 마지막 $는 string의 끝\n",
        "#result = [w for w in wsj if re.search('^[0-9]{4}$', w)] # {4} >> 정확히 4번 나옴\n",
        "#result = [w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)] # {3,5} >> 3개에서 5개 사이\n",
        "#result = [w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)] # {5,} >> 5개 이상; {,6} >> 6개 이하\n",
        "\n",
        "result = sorted(set(result))\n",
        "print(result[:10])"
      ],
      "metadata": {
        "id": "nEul8ZtroM0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "url = \"https://raw.githubusercontent.com/junkyuhufs/Class2022Fall/main/friends_season_01_script.txt\"\n",
        "os.system(\"curl \" + url + \" > friends_season01_script.txt\")\n",
        "\n",
        "# read a text file in the server\n",
        "file = open(\"friends_season01_script.txt\")\n",
        "text = file.read()\n",
        "file.close()\n",
        "text"
      ],
      "metadata": {
        "id": "sccBWsukoSRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'(?<=a).(?=b)'  >>> a로 시작하고 b로 끝나는 데, 그 안에 있는 character 하나 (dot)를 달라. 괄호로 좌우 환경 지정\n",
        "\n",
        ".+ >>> 하나 이상의 character가 나오면\n",
        "\n",
        "[\\.|\\?|\\!] >>> 마침표, 물음표, 느낌표가 있으면"
      ],
      "metadata": {
        "id": "OITQsQEdZsFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = '(?<=: ).+(?=[\\.|\\?|\\!])'\n",
        "sent = re.findall(pattern, text) # text 중에서 pattern을 만족하는 것을 sent에 담아라\n",
        "len(sent)\n",
        "sent[:50]"
      ],
      "metadata": {
        "id": "JEAEvgGIoYei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '\\n'.join(sent) # '\\n' >>> 줄바꿈을 기준으로 join해라\n",
        "text"
      ],
      "metadata": {
        "id": "elTJG3VxdNtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write out a text file\n",
        "file = open(\"tmp.txt\", \"w\")\n",
        "file.write(text)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "uotBXSAnocAb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}